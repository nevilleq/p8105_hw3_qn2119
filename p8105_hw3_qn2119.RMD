---
title: "P8105 Homework 3"
author: "Quinton Neville"
date: "Due October 15th, 2018"
header-includes: 
  \usepackage{graphicx}
  \usepackage{float}
output:
   github_document 
---

```{r setup, echo = FALSE, warning = FALSE, message = FALSE}
#Load necessary packages
library(tidyverse)
library(readxl)
library(readr)
library(p8105.datasets)
library(patchwork)
library(ggridges)
library(gridExtra)

#Controlling figure output in markdown
knitr::opts_chunk$set(
#  fig.height =   
  fig.width = 6,
#  fig.asp = .5,
  out.width = "90%",
#  out.height = 
  cache = TRUE
)

#Set Theme for ggplot2
theme_set(theme_bw() + theme(legend.position = "bottom"))

#Set Scientific notation output for knitr
options(scipen = 999999)

```

#Problem 1
This problem focuses on the BRFSS data. In order to analyze these data, we formatted the data by standardizing variable names, filtered by observations concerning the "Overall Health" topic, selected those responses from "Excellent" to "Poor", cast these variables as factors and re-ordered them from "Excellent" to "Poor". 

```{r warning = FALSE, message = FALSE, error = FALSE}
#Call data from p8105 library
data("brfss_smart2010")

#Read in & Clean brfss_smart2010
brfss.df <- brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  select(year, locationabbr, locationdesc, response, data_value) %>%
  mutate(locationdesc = substring(locationdesc, 6) %>% toupper(),                         #Remove redundant state, std. chars 
         locationabbr = locationabbr %>% toupper,
         response = as.factor(response),
         response = forcats::fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))) %>%
  rename(state = locationabbr, county = locationdesc, 
         proportion = data_value) 
```

We then completed/answered the following tasks/questions:

- In 2002, which states were observed at 7 locations?  
- Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.  
- Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.  
- For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.  

####Solutions
```{r warning = FALSE, message = FALSE, error = FALSE}
#States Observed at 7 locations in 2002
states.obs7.2002 <- brfss.df %>%
  filter(year == 2002) %>%
  distinct(., state, county) %>%
  count(state) %>%
  mutate(seven.logical = ifelse(n == 7, TRUE, FALSE)) %>%
  filter(seven.logical == TRUE) %>%
  select(state) %>%
  apply(., 2, paste0, collapse = ", ")
```
After filtering the data by those distinct state, county observations in 2002; we see that `r states.obs7.2002` were each observed at 7 distinct locations in 2002.

```{r warning = FALSE, message = FALSE, error = FALSE}
#Spaghetti Plot Num. Locations in Each Year 2002-2010
state.obs.02.10 <- brfss.df %>%
  distinct(., year, state, county) %>%
  count(year,state) %>%
  ggplot(aes(x = year, y = n, colour = state)) +
  labs(
    x = "Year",
    y = "Number of Observations",
    title = "Spaghetti Plot of N.Observations/Year by State"
  ) +
  viridis::scale_color_viridis(
    option = "magma",
    name = "State", 
    discrete = TRUE
  ) + 
  theme(legend.position = "right")

line.gg.02.10 <- state.obs.02.10 + geom_line(alpha = 0.8)
smooth.gg.02.10 <- state.obs.02.10 + geom_smooth(alpha = 0.8, se = FALSE)
```

We then obtained linear and smoothed spaghetti plots of the number of locations in each state from 2002 to 2010. We see that most states are observed at fewer than 10 distinct locations each years, while a few outlying states are observed at almost 20 distinct locations per year and one state was observed at over 40 locations in 2007 and 2010.
```{r echo = FALSE, warning = FALSE, message = FALSE, error = FALSE}
line.gg.02.10 
smooth.gg.02.10
```

Next, we produced a a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State. We see that there was very little change in percent "Excellent" responses from 2002 to 2006, but a significant increase in the mean percentage of respondants ellecting an "Excellent" response to "overall Health" in 2010, with a significant reduction in response variability.
```{r warning = FALSE}
  brfss.df %>%
  filter(year == 2002 | year == 2006 | year == 2010 & 
         response == "Excellent") %>%
  group_by(year) %>%
  summarize(mean_excellent = mean(proportion, na.rm = T),
            sd_excellent = sd(proportion, na.rm = T)) %>%
  knitr::kable()
```  

Lastly, for each year and state, we computed the average proportion in each response category (taking the average across locations in a state). Below, we obtained a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time. We observed a relatively stable temporal distribution of approximately 20-30% "Excellent", 30-40% "Very Good", 25-35% "Good", 5-15% "Fair", and 2.5-7.5% "Poor" responses by state per year. Additionally, the variability in proportion state responses per year in the "Excellent" - "Good" categories was quite high, while the variability for "Fair" and "Poor" responses was significantly lower. Finally, after fitting a linear model (in black), we see a downward trend in proportion of "Excellent" responses from 2002-2010, a slight increase in proportion of "Very Good" and "Good" reponses, and relatively little change in "Fair" and "Poor" responses over the same time period. 
```{r warning = FALSE}
#5 plot summary
response.st.yr.gg <- brfss.df %>%
  spread(key = response, value = proportion) %>%          #Create col. vars for % responses
            janitor::clean_names() %>%
  group_by(year, state) %>%
  summarize(Excellent = mean(excellent, na.rm = T),       #Calculate mean prop. response by year/state
            Very_good = mean(very_good, na.rm = T),
            Good = mean(good, na.rm = T),
            Fair = mean(fair, na.rm = T),
            Poor = mean(poor, na.rm = T)) %>%
  gather(key = response, value = mean, Excellent:Poor) %>%  #Return to tidy format
  mutate(response = forcats::fct_relevel(response, c("Excellent", "Very_good", "Good", "Fair", "Poor"))) %>% #Relevel
  ggplot(aes(x = year, y = mean, colour = state)) +
  facet_grid(~response) +
    viridis::scale_color_viridis(
    option = "magma",
    name = "State", 
    discrete = TRUE
  ) + 
    labs(
    x = "Year",
    y = "Mean Number of Observations",
    title = "Spaghetti Plot of N.Observations/Year by State"
  ) +
  theme(legend.position = "none",
        axis.text.x = element_text(color = "black", 
        size = 10, angle = 430, vjust = .5) )

response.st.yr.gg + geom_line() + geom_smooth(method = "lm", alpha = 0.9, size = 0.75, colour = "black", se = F)
```

#Problem 2
This problem focuses on the Instacart data, found in the P8105 Github Repository.

First, we will provide a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illustrative examples of observations. Then we will do or answer the following:

- How many aisles are there, and which aisles are the most items ordered from?  
- Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.  
- Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.  
- Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).  

```{r warning = FALSE, error = FALSE, message = FALSE, cache = TRUE}
#Load data from p8105 datasets
data("instacart")

#Pull the dimmensions
dim.insta.cart <- dim(instacart)

#Clean Names
insta.cart.df <- instacart %>%
  janitor::clean_names() 

#Summary of Customer Visits (tibble for display and inline access)
sum.df <- insta.cart.df %>%
  janitor::clean_names() %>%
  group_by(order_id, user_id) %>%
  summarize(
    n_items = max(add_to_cart_order),
    n_unique_aisles = aisle_id %>% unique() %>% length(),
    n_unique_dept = department %>% unique() %>% length(),
    n_days_last_order = unique(days_since_prior_order),
    prct_reorder = mean(reordered),
    order_num = unique(order_number)
    ) %>% gather(key = Customer_Stats, value = count, n_items:order_num) %>%
  group_by(Customer_Stats) %>%
  summarize(
    median = median(count, na.rm = T),
    mean = mean(count, na.rm = T),
    variance = sd(count, na.rm = T)^2,
    max = max(count),
    min = min(count)
    )

#Number of Unique products bought   
unique.products <- insta.cart.df %>% distinct(., product_id) %>% nrow()

```
#Data Overview
Upon intial exploration, the Instacart data set contains `r dim.insta.cart[1]` observations in `r dim.insta.cart[2]` variables, where each observation describes an individual grocery item purchase made via instacart and the variables describe features of each purchase. Such features include product name, id, aisle of purchase, department of purchase, purchase day of the week, purchase time, whether or not the purchase was a reorder, customer id, purchase number by customer order, and visit number by customer. Overall, `r unique.products` unique products were purchased in these data, and on average, each customer purchased `r round(sum.df[2, 'mean'])` items, of which `r 100*round(sum.df[6, 'mean'],4)`% were reordered, and visited an average of `r round(sum.df[3, 'mean'])` unique aisles from `r round(sum.df[4, 'mean'])` unique departments. The median customer was shopping at Instacart again `r round(sum.df[1, 'mean'])` days after their previous order, for their `r round(sum.df[5, 'median'])`th time. However, there existed a group of heavy outlying customers who were on nearly their 100th visit, implying most people use the service regularly but infrequently while a select group use it both regularly and frequently. Additionally, the number of items purches per order, days since last order, and the order number, elicited a high degree of variability amongst customers. (Full table found below)

```{r echo = FALSE}
sum.df %>% knitr::kable()
```

```{r echo = FALSE}
user.310.df <- insta.cart.df %>%
  filter(user_id == 310)
```
As an illustrative example of what these Instacart data actually describe, a group of purchase observations from customer `r user.310.df$user_id[1]`'s `r user.310.df$order_number[1]`th visit tell us that this customer ordered `r nrow(user.310.df)` items (`r paste0(user.310.df$product_name, collapse = ", ")`), of which `r user.310.df %>% filter(reordered == 1) %>% select(product_name)` was a reorder. These purchases were made from the `r paste0(user.310.df$aisle, collapse = ", ")` aisles in the `r paste0(user.310.df$department, collapse = ", ")` departments. Additionally, these purchases took place on Tuesday at 9 a.m. and it had been `r user.310.df$days_since_prior_order[1]` days since their last order. Together, this customer's total order comprised `r nrow(user.310.df)` purchase observations in the Instacart dataset.

####Solutions
```{r}
#Top 5 Aisles
n.unique.aisles <- insta.cart.df %>% distinct(., aisle) %>% nrow()
top.5.aisles.df <- insta.cart.df %>%
  count(aisle) %>% arrange(desc(n)) %>% slice(1:5) %>%
  select(aisle)
```

After some small data manipulations, we obtained that there exist `r n.unique.aisles` distinct aisles in the Instacart data. Of these,  `r paste0(top.5.aisles.df$aisle, collapse = ", ")` were the top 5 aisles from which products were ordered. As these are highly perishable items, it makes some sense that these are the items ordered most frequently as they go bad the fastest. Below, we obtained an exhaustive barplot of the number of items ordered by aisle.

```{r fig.width = 8, fig.height = 14}
#Histogram of Order in each aisle
hist.aisle.orders <- insta.cart.df %>%
  count(aisle, department) %>% arrange(desc(n)) %>%
  mutate(aisle = as.factor(aisle)) %>%
  mutate(aisle = forcats::fct_reorder(aisle, n)) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_bar(stat = "identity", width = 1, fill = "lightblue", colour = "black") +
    coord_flip() +
  labs(
    x = "Aisle",
    y = "Number of Orders",
    title = "Number of Orders by Aisle"
  ) +
    theme(legend.position = "right",
        axis.text.y = element_text(color = "black", 
        size = 10,  hjust = 1) ) 

hist.aisle.orders
```

Next, we obtained a table of the most popular items ordered from the “baking ingredients”, “dog food care”, and “packaged vegetables fruits” aisles, respectively. Light Brown sugar is a very common baking ingredient, and makes sense that it would be the most purchased baking ingredient. The specific dog treats purchased most in the dog food care aisle would not be immediately obvious, but apparently these were the most popular dog food treat. Lastly, spinach is a very popular vegetable purchase amongst current younger demographics prone to ordering groceries over the internet, so the fact that spinach was the most popular purchase in the packaged vegetables aisle is not wholly unsurprising. (Table found below)
```{r}
#Most popular item in “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
pop.item.aisle.df <- insta.cart.df %>%
  filter(aisle == "baking ingredients" | 
         aisle == "dog food care" | 
         aisle == "packaged vegetables fruits") %>%
  count(product_name, aisle) %>%
  group_by(aisle) %>%
  summarize(
    'Most Popular Product' = product_name[which.max(n)],
    'Number Bought' = max(n)
  ) %>%
  rename(Aisle = aisle)
pop.item.aisle.df %>%
  knitr::kable()
```

Lastly, we obtained a table of the mean hour of the day (12 hour, a.m./p.m.), per each day of the week, that both Pink Lady Apples and Coffee Ice Cream are purchased. Here we assumed that for days coded (0-6), that 0 corresponded to Sunday, and the rest followed suit until 6 - Saturday. We noticed that on average, these two items were most often purchased in the early afternoon, but significantly later on Wednesdays than any other day of the week. (Table found below)
```{r}
#Mean hour Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers
#User function to change mean time (decimal, 24 hour) to 12 hour time (hour:min a.m./p.m.)
#Inputs numeric vector, outputs character vector of same length
dec.to.min <- function(x){
  for (i in 1:length(x)) {  #loop through each vector element
      y <- as.character(x[i])    #cast as character
      y1 <- sapply(strsplit(y, "[.]"), `[`, 1) %>% as.numeric()  #split on decimal,pull hour, cast as numeric
      y2 <- substring(x[i],3) %>% as.numeric()*60 #pull decimal, convert to minutes
    if (y2 < 10) {
        y2 <- paste0(0,round(y2))  #if less than 10, add "0_" and round to integer
    } else {
        y2 <- round(y2) #round to integer
      }
    if (y1 >= 13) {
      y1 <- y1 - 12   #convert to 24 hour if p.m.
      x[i] <- paste(y1, y2, sep = ":") %>%  #replace ith entry with "__:__ p.m." if hour > 13
          paste(., "p.m.")
    } else if (y1 == 12) {
      x[i] <- paste(y1, y2, sep = ":") %>% # #replace ith entry with "__:__ p.m." if hour = 12
          paste(., "p.m.")
    } else {
      x[i] <- paste(y1, y2, sep = ":") %>% #replace ith entry with "__:__ a.m." if hour < 12.
          paste(., "a.m.")
    }
  }
  return(x)
}
#Final Table for Display
  insta.cart.df %>%
  filter(product_name == "Pink Lady Apples" |
         product_name == "Coffee Ice Cream") %>%
  group_by(order_dow) %>%
  summarize(
    mean_hour = mean(order_hour_of_day)
  ) %>%
  mutate(
    mean_hour = dec.to.min(mean_hour),
    order_dow = c("Sunday", "Monday", "Tuesday", "Wednesday",
                  "Thursday", "Friday", "Saturday")
  ) %>%
  rename('Order Day of the Week' = order_dow,
         'Mean Time of Order' = mean_hour) %>%
  knitr::kable()
```

#Problem 3
This problem focuses on the NY NOAA data. To begin, we will do some data cleaning, creating separate variables for year, month, and day, and standardizing observation units for temperature, precipitation, and snowfall (mm, degrees Celsius), respectively.
```{r message = FALSE, warning = FALSE, error = FALSE, cache = TRUE}
#Load Data from P8105 git repo
data("ny_noaa")

#Dimensions(original)
original.dim.noaa <- dim(ny_noaa)

#Clean NY NOAA data
ny.noaa.df <- ny_noaa %>%
  janitor::clean_names() %>%
  mutate(
    year = date %>% as.character() %>% strsplit(., "[-]") %>% sapply(., `[`, 1) %>% as.numeric(),  #String pull year
    month = date %>% as.character() %>% strsplit(., "[-]") %>% sapply(., `[`, 2) %>% as.numeric(), #String pull month
    day = date %>% as.character() %>% strsplit(., "[-]") %>% sapply(., `[`, 3) %>% as.numeric(),   #String pull day
    month =  recode(month, '1'  = "January", '2' = "February", '3' = "March", '4' = "April",       #Factor, relevel month by name
         '5' = "May", '6' = "June", '7' = "July", '8' = "August", '9' = "September",
         '10' = "October", '11' = "November", '12' = "December") %>% 
    as.factor() %>% forcats::fct_relevel(., month.name),
    tmax = tmax %>% as.numeric()/10, #degrees c     
    tmin = tmin %>% as.numeric()/10, #degrees c
    prcp = prcp/10 #mm
  ) %>%
  select(id, year, month, day, everything(), -date) 

#Missing Data
missing.prct <- ny.noaa.df %>%
  select(prcp:tmin) %>%
  apply(., 2, is.na) %>%
  apply(., 2, mean) %>%
  round(., 4) * 100
```

Next, we will provide a short description of the dataset, noting the size and structure of the data, describing some key variables, and discuss the extent to which missing data is an issue. Then, we will do or answer the following:

- For snowfall, what are the most commonly observed values? Why?  
- Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?  
-  Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

####Data Overview
Considering the NYC NOAA data, there were `r original.dim.noaa[1]` weather station observations in `r original.dim.noaa[2]` variables. Here, each observation is a report from an individual NYC weather station to the NOAA, and the variables describe the year, month, and day of observation, the precipitation, snowfall, and snow depth in millimeters, as well as the maximum and minimum temperature in degrees Celsius. From the original data, precipitation, snowfall, snow depth, max temp., and min. temp are each missing to varying degrees (`r paste0(missing.prct, collapse = "%, ")`%), respectively. Missing precipitation for about 5% of the data is not a huge problem, but for the remaining variables, the high degree to which the information is missing does not allow us to assume that these data were missing at random, and severly limits the degree to which we could draw inferential conclusions from the data. 

####Solutions
```{r message = FALSE, warning = FALSE, error = FALSE, cache = TRUE}
#Most common snowfall count
most.obs.snow <- ny.noaa.df %>% na.omit() %>%
  mutate(snow = snow %>% as.character()) %>%  #Zero, for obvious reasons, it doesn't snow that often in nyc
  count(snow) %>% arrange(desc(n))  %>%
  slice(1:5) %>%
  rename('Snowfall (mm)' = snow,
         'Number of Observations' = n)
```

With respect to snowfall, we obtained that the most common observation by far was 0mm of snowfall reported by a NYC weather station. This is not wholly unsurprising as it does not snow that frequently in NYC, even during the winter months. However, it is interesting to note that the next most common observation was 25mm of snowfall, which is not a trivial amount of snowfall. A table of the 5 most common snowfall observations can be found below.
```{r echo = FALSE}
most.obs.snow %>% knitr::kable()
```

Next, we obtained a two panel plot for the average maximum temperature in degrees Celsius reported by each station from 1981-2010, for the months of January and July, respectively. We observed that the average maximum temperature was much more variable in January than in July each year, meaning that the average station reported a much wider range of maximum temperatures (more outliers) in the winter than in the summer within each year. Additionally, from year to year, the average maximum temperature reported varied much more in the winter than in the summer, with a small but significant increase in overall average maximum temperature across all stations in January and little to no observably significant change in July from 1981-2010. This would suggest that average maxmimum temperatures reported across all stations were much more variable and increased in January from 1981-2010, while there was much less variability and relatively little change in July over the same time period. 
```{r}
#Two panel plot of avg max temp in January and July by station over the years
gg.avg.temp <- ny.noaa.df %>%
  na.omit() %>% filter(month == "January" | month == "July") %>%
  group_by(id, year, month) %>%
  summarize(
    avg_max_temp = mean(tmax) %>% round(., 3)
  ) %>%
  ggplot(aes(x = year, y = avg_max_temp, colour = id)) +
    facet_grid(~month) +
    viridis::scale_color_viridis(
    option = "magma",
    name = "Station", 
    discrete = TRUE
  ) + 
    labs(
    x = "Year",
    y = expression(paste("Average Max Temperature ( ", degree ~ C, ")")),
    title = "Plots of Avg Max Temp/Year by Station, (Jan., July)"
  ) +
  theme(legend.position = "none",
        axis.text.x = element_text(color = "black", 
        size = 10, angle = 45, vjust = .5)) +
  scale_x_continuous(breaks = seq(1980, 2010, 4))

gg.avg.temp + 
  geom_line(alpha = 0.3) +
  geom_smooth(method = "lm", colour = "black", size = 0.75, alpha = 0.9)
```

Lastly, we created a two panel plot showing the (1) Density of minimum and maximum temperatures reported across all stations over all years in a binned hex-plot and (2) Distribution of snowfall between 0 and 100 mm each year from 1981-2010. We observed from panel (1), that the highest concentration of minimum and maximum temperatures reported fell between -5 to 25 degrees at maximum and -20 to 20 degrees at minimum, with an entire range of aboute -40 to 40 for both (with a few warmer outliers for maximum temperature). With respect to panel (2), we observed that the distribution of snowfall from the years 1981-2010 did not vary significantly from year to year. Additionally, each year the distribution exhibited a heavy right skew, with a majority of stations reporting between 0-25mm and a few clusterings around 50 and 80mm each year. 

```{r fig.width = 8, fig.height = 10, out.width = "90%", fig.asp = 1, message = FALSE, warning = FALSE, error = FALSE}
# (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option)
  gg.min.max <- ny.noaa.df %>%
  na.omit() %>% ggplot(aes(x = tmin, y = tmax)) +
    viridis::scale_fill_viridis(
    option = "magma",
    name = "Count", 
    discrete = FALSE
  ) + theme(
    legend.text = element_text(size = 10, angle = 45, hjust = 1),
    legend.title = element_text(size = 12, vjust = 1)
  ) +
  labs(
    x = expression(paste("Minimum Temperature ( ", degree ~ C, ")")),
    y = expression(paste("Maximum Temperature ( ", degree ~ C, ")")),
    title = "Max. and Min. Temp., NYC NOAA"
  ) +
  geom_hex(bins = 42) +
  scale_y_continuous(breaks = c(seq(-60, 60, 20)))
  
#(ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
gg.snow.year <- ny.noaa.df %>%
  na.omit() %>% filter(snow < 100 & snow > 0) %>%
  mutate(year = as.factor(year)) %>%
ggplot(aes(x = snow, y = year)) + 
  geom_density_ridges(fill = "lightblue", scale = 1) +
    theme(legend.position = "none") +
  labs(
    x = "Snowfall (mm)",
    y = "Year",
    title = "Snowfall by Year, NYC NOAA"
  ) +
  scale_x_continuous(breaks = seq(0, 100, 20))

  #Arrange in nice 2 panel grid
grid.arrange(gg.min.max, gg.snow.year, nrow = 1, ncol = 2)
```
